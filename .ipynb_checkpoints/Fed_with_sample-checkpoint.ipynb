{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.use('Agg')\n",
    "%matplotlib inline\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.fc1 = nn.Linear(2048, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "\n",
    "def client_update(client_model, optimizer, train_loader, epoch, num_clients, pk, batch_size):\n",
    "    client_model.train()\n",
    "    Grad_accumulator = []\n",
    "    for e in range(epoch):\n",
    "        grad_batch_idx = []\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            output = client_model(data)\n",
    "            loss_fed = F.nll_loss(output, target)\n",
    "            loss = loss_fed/(epoch *num_clients*pk)\n",
    "            loss.backward()\n",
    "            #print(type(client_model))\n",
    "            #print(dir(client_model))\n",
    "#             print(client_model.parameters())\n",
    "#             grad_batch_idx.append(client_model.parameters().grad.numpy()) \n",
    "#             grad_batch_idx.append(list(i.grad for i in list(client_model.parameters())))\n",
    "            if Grad_accumulator == []:\n",
    "                Grad_accumulator = list(i.grad for i in list(client_model.parameters()))\n",
    "                    \n",
    "            else:\n",
    "                h = list(i.grad for i in list(client_model.parameters()))\n",
    "                Grad_accumulator = [Grad_accumulator[i]+h[i] for i in range(len(Grad_accumulator))]\n",
    "            optimizer.step()\n",
    "#         grad_batch_idx_np = np.array(grad_batch_idx)\n",
    "#         grad_batch_e = sum(grad_batch_idx_np)/len(grad_batch_idx_np)\n",
    "#         grad_epoch.append(grad_batch_e)\n",
    "#     grad_client = sum(grad_epoch)/epoch \n",
    "#     grad_client_tensor = torch.from_numpy(grad_client)\n",
    "    \n",
    "    nabla_P_norm2 = sum([(torch.norm(a))**2 for a in Grad_accumulator]).item()\n",
    "    grad_client = (1/(epoch*batch_size))*np.sqrt(nabla_P_norm2)\n",
    "    return loss_fed.item(),grad_client\n",
    "\n",
    "def server_aggregate(global_model, client_models):\n",
    "    global_dict = global_model.state_dict()\n",
    "    for k in global_dict.keys():\n",
    "        global_dict[k] = torch.stack([client_models[i].state_dict()[k] for i in range(len(client_models))], 0).mean(0)\n",
    "    global_model.load_state_dict(global_dict)\n",
    "    for model in client_models:\n",
    "        model.load_state_dict(global_model.state_dict())\n",
    "\n",
    "def test(global_model, test_loader):\n",
    "    global_model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            output = global_model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    acc = correct / len(test_loader.dataset)\n",
    "\n",
    "    return test_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-th round\n",
      "average train loss 2.14 | test loss 2.3 | test acc: 0.089\n",
      "1-th round\n",
      "average train loss 2.03 | test loss 2.3 | test acc: 0.089\n",
      "2-th round\n",
      "average train loss 1.64 | test loss 2.31 | test acc: 0.133\n",
      "3-th round\n",
      "average train loss 0.961 | test loss 2.43 | test acc: 0.210\n",
      "4-th round\n",
      "average train loss 0.772 | test loss 2.48 | test acc: 0.154\n",
      "5-th round\n",
      "average train loss 0.661 | test loss 2.53 | test acc: 0.198\n",
      "6-th round\n",
      "average train loss 0.609 | test loss 2.67 | test acc: 0.192\n",
      "7-th round\n",
      "average train loss 0.556 | test loss 2.36 | test acc: 0.103\n",
      "8-th round\n",
      "average train loss 0.507 | test loss 2.27 | test acc: 0.175\n",
      "9-th round\n",
      "average train loss 0.541 | test loss 3.02 | test acc: 0.180\n",
      "10-th round\n",
      "average train loss 0.367 | test loss 2.06 | test acc: 0.436\n",
      "11-th round\n",
      "average train loss 0.271 | test loss 2.15 | test acc: 0.369\n",
      "12-th round\n",
      "average train loss 0.324 | test loss 2.04 | test acc: 0.416\n",
      "13-th round\n",
      "average train loss 0.276 | test loss 1.88 | test acc: 0.248\n",
      "14-th round\n",
      "average train loss 0.258 | test loss 1.91 | test acc: 0.307\n",
      "15-th round\n",
      "average train loss 0.234 | test loss 1.75 | test acc: 0.354\n",
      "16-th round\n",
      "average train loss 0.219 | test loss 1.76 | test acc: 0.379\n",
      "17-th round\n",
      "average train loss 0.199 | test loss 1.58 | test acc: 0.459\n",
      "18-th round\n",
      "average train loss 0.265 | test loss 1.76 | test acc: 0.485\n",
      "19-th round\n",
      "average train loss 0.169 | test loss 2.03 | test acc: 0.282\n",
      "20-th round\n",
      "average train loss 0.158 | test loss 1.67 | test acc: 0.396\n",
      "21-th round\n",
      "average train loss 0.0801 | test loss 1.93 | test acc: 0.408\n",
      "22-th round\n",
      "average train loss 0.107 | test loss 1.37 | test acc: 0.517\n",
      "23-th round\n",
      "average train loss 0.127 | test loss 1.96 | test acc: 0.222\n",
      "24-th round\n",
      "average train loss 0.0918 | test loss 1.39 | test acc: 0.484\n",
      "25-th round\n",
      "average train loss 0.149 | test loss 1.31 | test acc: 0.546\n",
      "26-th round\n",
      "average train loss 0.144 | test loss 1.22 | test acc: 0.580\n",
      "27-th round\n",
      "average train loss 0.118 | test loss 1.59 | test acc: 0.362\n",
      "28-th round\n",
      "average train loss 0.14 | test loss 1.16 | test acc: 0.567\n",
      "29-th round\n",
      "average train loss 0.0827 | test loss 1.12 | test acc: 0.599\n",
      "30-th round\n",
      "average train loss 0.0601 | test loss 1.7 | test acc: 0.361\n",
      "31-th round\n",
      "average train loss 0.103 | test loss 1.13 | test acc: 0.608\n",
      "32-th round\n",
      "average train loss 0.0652 | test loss 1.17 | test acc: 0.596\n",
      "33-th round\n",
      "average train loss 0.0902 | test loss 1.28 | test acc: 0.485\n",
      "34-th round\n",
      "average train loss 0.113 | test loss 1.08 | test acc: 0.638\n",
      "35-th round\n",
      "average train loss 0.103 | test loss 1.39 | test acc: 0.478\n",
      "36-th round\n",
      "average train loss 0.0871 | test loss 1.1 | test acc: 0.592\n",
      "37-th round\n",
      "average train loss 0.064 | test loss 0.963 | test acc: 0.676\n",
      "38-th round\n",
      "average train loss 0.071 | test loss 0.941 | test acc: 0.697\n",
      "39-th round\n",
      "average train loss 0.0636 | test loss 1.26 | test acc: 0.504\n",
      "40-th round\n",
      "average train loss 0.0665 | test loss 1.23 | test acc: 0.561\n",
      "41-th round\n",
      "average train loss 0.0385 | test loss 1.31 | test acc: 0.529\n",
      "42-th round\n",
      "average train loss 0.0547 | test loss 1.34 | test acc: 0.521\n",
      "43-th round\n",
      "average train loss 0.1 | test loss 1.01 | test acc: 0.632\n",
      "44-th round\n",
      "average train loss 0.0682 | test loss 1.09 | test acc: 0.581\n",
      "45-th round\n",
      "average train loss 0.0731 | test loss 1.03 | test acc: 0.639\n",
      "46-th round\n",
      "average train loss 0.0906 | test loss 1.06 | test acc: 0.597\n",
      "47-th round\n",
      "average train loss 0.074 | test loss 0.96 | test acc: 0.653\n",
      "48-th round\n",
      "average train loss 0.0773 | test loss 1 | test acc: 0.638\n",
      "49-th round\n",
      "average train loss 0.0387 | test loss 1.05 | test acc: 0.608\n",
      "50-th round\n",
      "average train loss 0.0879 | test loss 0.938 | test acc: 0.675\n",
      "51-th round\n",
      "average train loss 0.0409 | test loss 0.937 | test acc: 0.671\n",
      "52-th round\n",
      "average train loss 0.0853 | test loss 1.17 | test acc: 0.532\n",
      "53-th round\n",
      "average train loss 0.075 | test loss 0.853 | test acc: 0.718\n",
      "54-th round\n",
      "average train loss 0.0359 | test loss 1.21 | test acc: 0.574\n",
      "55-th round\n",
      "average train loss 0.0462 | test loss 1.04 | test acc: 0.618\n",
      "56-th round\n",
      "average train loss 0.0985 | test loss 1.05 | test acc: 0.587\n",
      "57-th round\n",
      "average train loss 0.062 | test loss 0.784 | test acc: 0.738\n",
      "58-th round\n",
      "average train loss 0.0856 | test loss 1.15 | test acc: 0.640\n",
      "59-th round\n",
      "average train loss 0.044 | test loss 1.02 | test acc: 0.632\n",
      "60-th round\n",
      "average train loss 0.0689 | test loss 1.21 | test acc: 0.537\n",
      "61-th round\n",
      "average train loss 0.0518 | test loss 0.944 | test acc: 0.635\n",
      "62-th round\n",
      "average train loss 0.0937 | test loss 0.839 | test acc: 0.692\n",
      "63-th round\n",
      "average train loss 0.0682 | test loss 0.923 | test acc: 0.685\n",
      "64-th round\n",
      "average train loss 0.0287 | test loss 0.8 | test acc: 0.722\n",
      "65-th round\n",
      "average train loss 0.0934 | test loss 0.807 | test acc: 0.699\n",
      "66-th round\n",
      "average train loss 0.0533 | test loss 0.854 | test acc: 0.689\n",
      "67-th round\n",
      "average train loss 0.0515 | test loss 1.04 | test acc: 0.656\n",
      "68-th round\n",
      "average train loss 0.0736 | test loss 1.07 | test acc: 0.624\n",
      "69-th round\n",
      "average train loss 0.0651 | test loss 0.841 | test acc: 0.696\n",
      "70-th round\n",
      "average train loss 0.0684 | test loss 1.37 | test acc: 0.562\n",
      "71-th round\n",
      "average train loss 0.0858 | test loss 1.58 | test acc: 0.450\n",
      "72-th round\n",
      "average train loss 0.0515 | test loss 0.826 | test acc: 0.696\n",
      "73-th round\n",
      "average train loss 0.0376 | test loss 0.726 | test acc: 0.762\n",
      "74-th round\n",
      "average train loss 0.0627 | test loss 1.11 | test acc: 0.598\n",
      "75-th round\n",
      "average train loss 0.0429 | test loss 0.813 | test acc: 0.703\n",
      "76-th round\n",
      "average train loss 0.0816 | test loss 0.934 | test acc: 0.650\n",
      "77-th round\n",
      "average train loss 0.0323 | test loss 0.717 | test acc: 0.763\n",
      "78-th round\n",
      "average train loss 0.0401 | test loss 0.739 | test acc: 0.760\n",
      "79-th round\n",
      "average train loss 0.0645 | test loss 1.12 | test acc: 0.623\n",
      "80-th round\n",
      "average train loss 0.0435 | test loss 0.675 | test acc: 0.782\n",
      "81-th round\n",
      "average train loss 0.0437 | test loss 0.754 | test acc: 0.733\n",
      "82-th round\n",
      "average train loss 0.0377 | test loss 1.14 | test acc: 0.600\n",
      "83-th round\n",
      "average train loss 0.0368 | test loss 0.736 | test acc: 0.746\n",
      "84-th round\n",
      "average train loss 0.0438 | test loss 0.824 | test acc: 0.702\n",
      "85-th round\n",
      "average train loss 0.0674 | test loss 0.693 | test acc: 0.764\n",
      "86-th round\n",
      "average train loss 0.0449 | test loss 0.927 | test acc: 0.669\n",
      "87-th round\n",
      "average train loss 0.034 | test loss 0.889 | test acc: 0.668\n",
      "88-th round\n",
      "average train loss 0.051 | test loss 0.964 | test acc: 0.669\n",
      "89-th round\n",
      "average train loss 0.0558 | test loss 1.11 | test acc: 0.664\n",
      "90-th round\n",
      "average train loss 0.0533 | test loss 0.965 | test acc: 0.671\n",
      "91-th round\n",
      "average train loss 0.0379 | test loss 0.807 | test acc: 0.717\n",
      "92-th round\n",
      "average train loss 0.0438 | test loss 0.836 | test acc: 0.693\n",
      "93-th round\n",
      "average train loss 0.0259 | test loss 1.11 | test acc: 0.606\n",
      "94-th round\n",
      "average train loss 0.0403 | test loss 0.993 | test acc: 0.614\n",
      "95-th round\n",
      "average train loss 0.0395 | test loss 0.706 | test acc: 0.752\n",
      "96-th round\n",
      "average train loss 0.0373 | test loss 0.932 | test acc: 0.681\n",
      "97-th round\n",
      "average train loss 0.0503 | test loss 0.698 | test acc: 0.769\n",
      "98-th round\n",
      "average train loss 0.0701 | test loss 0.745 | test acc: 0.734\n",
      "99-th round\n",
      "average train loss 0.0514 | test loss 0.968 | test acc: 0.642\n",
      "100-th round\n",
      "average train loss 0.0625 | test loss 0.802 | test acc: 0.709\n",
      "101-th round\n",
      "average train loss 0.0669 | test loss 0.741 | test acc: 0.749\n",
      "102-th round\n",
      "average train loss 0.0293 | test loss 0.787 | test acc: 0.702\n",
      "103-th round\n",
      "average train loss 0.0618 | test loss 0.672 | test acc: 0.783\n",
      "104-th round\n",
      "average train loss 0.073 | test loss 0.815 | test acc: 0.708\n",
      "105-th round\n",
      "average train loss 0.0965 | test loss 0.661 | test acc: 0.786\n",
      "106-th round\n",
      "average train loss 0.0425 | test loss 0.676 | test acc: 0.772\n",
      "107-th round\n",
      "average train loss 0.0839 | test loss 0.839 | test acc: 0.698\n",
      "108-th round\n",
      "average train loss 0.0496 | test loss 0.913 | test acc: 0.659\n",
      "109-th round\n",
      "average train loss 0.0773 | test loss 0.803 | test acc: 0.726\n",
      "110-th round\n",
      "average train loss 0.0288 | test loss 0.758 | test acc: 0.734\n",
      "111-th round\n",
      "average train loss 0.0593 | test loss 0.583 | test acc: 0.821\n",
      "112-th round\n",
      "average train loss 0.0458 | test loss 0.588 | test acc: 0.819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113-th round\n",
      "average train loss 0.0583 | test loss 0.559 | test acc: 0.828\n",
      "114-th round\n",
      "average train loss 0.0408 | test loss 0.609 | test acc: 0.804\n",
      "115-th round\n",
      "average train loss 0.0206 | test loss 0.625 | test acc: 0.796\n",
      "116-th round\n",
      "average train loss 0.021 | test loss 0.574 | test acc: 0.817\n",
      "117-th round\n",
      "average train loss 0.0425 | test loss 0.696 | test acc: 0.764\n",
      "118-th round\n",
      "average train loss 0.0494 | test loss 0.578 | test acc: 0.814\n",
      "119-th round\n",
      "average train loss 0.0199 | test loss 0.767 | test acc: 0.731\n",
      "120-th round\n",
      "average train loss 0.0956 | test loss 0.862 | test acc: 0.690\n",
      "121-th round\n",
      "average train loss 0.0203 | test loss 0.732 | test acc: 0.737\n",
      "122-th round\n",
      "average train loss 0.0717 | test loss 0.734 | test acc: 0.735\n",
      "123-th round\n",
      "average train loss 0.0444 | test loss 1.04 | test acc: 0.645\n",
      "124-th round\n",
      "average train loss 0.03 | test loss 0.684 | test acc: 0.761\n",
      "125-th round\n",
      "average train loss 0.0312 | test loss 0.582 | test acc: 0.798\n",
      "126-th round\n",
      "average train loss 0.0398 | test loss 0.666 | test acc: 0.762\n",
      "127-th round\n",
      "average train loss 0.0216 | test loss 1.01 | test acc: 0.659\n",
      "128-th round\n",
      "average train loss 0.0544 | test loss 0.58 | test acc: 0.819\n",
      "129-th round\n",
      "average train loss 0.0383 | test loss 0.651 | test acc: 0.777\n",
      "130-th round\n",
      "average train loss 0.0806 | test loss 0.613 | test acc: 0.797\n",
      "131-th round\n",
      "average train loss 0.0293 | test loss 0.527 | test acc: 0.830\n",
      "132-th round\n",
      "average train loss 0.022 | test loss 1.02 | test acc: 0.631\n",
      "133-th round\n",
      "average train loss 0.0561 | test loss 0.96 | test acc: 0.694\n",
      "134-th round\n",
      "average train loss 0.0295 | test loss 0.808 | test acc: 0.724\n",
      "135-th round\n",
      "average train loss 0.0245 | test loss 1.04 | test acc: 0.601\n",
      "136-th round\n",
      "average train loss 0.0342 | test loss 1.51 | test acc: 0.604\n",
      "137-th round\n",
      "average train loss 0.0128 | test loss 0.736 | test acc: 0.751\n",
      "138-th round\n",
      "average train loss 0.0386 | test loss 0.918 | test acc: 0.669\n",
      "139-th round\n",
      "average train loss 0.0622 | test loss 1.03 | test acc: 0.660\n",
      "140-th round\n",
      "average train loss 0.0534 | test loss 0.907 | test acc: 0.700\n",
      "141-th round\n",
      "average train loss 0.0422 | test loss 0.584 | test acc: 0.804\n",
      "142-th round\n",
      "average train loss 0.0388 | test loss 0.559 | test acc: 0.832\n",
      "143-th round\n",
      "average train loss 0.0171 | test loss 0.71 | test acc: 0.758\n",
      "144-th round\n",
      "average train loss 0.0652 | test loss 0.598 | test acc: 0.801\n",
      "145-th round\n",
      "average train loss 0.0156 | test loss 0.616 | test acc: 0.787\n",
      "146-th round\n",
      "average train loss 0.0215 | test loss 0.614 | test acc: 0.799\n",
      "147-th round\n",
      "average train loss 0.0944 | test loss 0.918 | test acc: 0.652\n",
      "148-th round\n",
      "average train loss 0.0664 | test loss 0.761 | test acc: 0.725\n",
      "149-th round\n",
      "average train loss 0.0281 | test loss 0.697 | test acc: 0.770\n",
      "150-th round\n",
      "average train loss 0.0278 | test loss 0.589 | test acc: 0.810\n",
      "151-th round\n",
      "average train loss 0.0605 | test loss 0.531 | test acc: 0.841\n",
      "152-th round\n",
      "average train loss 0.025 | test loss 0.715 | test acc: 0.739\n",
      "153-th round\n",
      "average train loss 0.0506 | test loss 0.623 | test acc: 0.783\n",
      "154-th round\n",
      "average train loss 0.0298 | test loss 0.68 | test acc: 0.773\n",
      "155-th round\n",
      "average train loss 0.0588 | test loss 0.63 | test acc: 0.784\n",
      "156-th round\n",
      "average train loss 0.0414 | test loss 0.826 | test acc: 0.694\n",
      "157-th round\n",
      "average train loss 0.0405 | test loss 0.894 | test acc: 0.646\n",
      "158-th round\n",
      "average train loss 0.0212 | test loss 0.631 | test acc: 0.771\n",
      "159-th round\n",
      "average train loss 0.0443 | test loss 0.615 | test acc: 0.785\n",
      "160-th round\n",
      "average train loss 0.0243 | test loss 0.587 | test acc: 0.805\n",
      "161-th round\n",
      "average train loss 0.0524 | test loss 0.634 | test acc: 0.787\n",
      "162-th round\n",
      "average train loss 0.122 | test loss 0.55 | test acc: 0.828\n",
      "163-th round\n",
      "average train loss 0.0581 | test loss 0.802 | test acc: 0.709\n",
      "164-th round\n",
      "average train loss 0.0599 | test loss 0.914 | test acc: 0.661\n",
      "165-th round\n",
      "average train loss 0.0781 | test loss 0.628 | test acc: 0.793\n",
      "166-th round\n",
      "average train loss 0.0242 | test loss 0.875 | test acc: 0.703\n",
      "167-th round\n",
      "average train loss 0.0294 | test loss 0.562 | test acc: 0.820\n",
      "168-th round\n",
      "average train loss 0.0173 | test loss 0.676 | test acc: 0.756\n",
      "169-th round\n",
      "average train loss 0.0226 | test loss 0.536 | test acc: 0.828\n",
      "170-th round\n",
      "average train loss 0.0889 | test loss 0.945 | test acc: 0.674\n",
      "171-th round\n",
      "average train loss 0.0503 | test loss 0.62 | test acc: 0.804\n",
      "172-th round\n",
      "average train loss 0.0322 | test loss 0.757 | test acc: 0.748\n",
      "173-th round\n",
      "average train loss 0.0908 | test loss 0.7 | test acc: 0.761\n",
      "174-th round\n",
      "average train loss 0.0663 | test loss 0.761 | test acc: 0.724\n",
      "175-th round\n",
      "average train loss 0.0439 | test loss 0.702 | test acc: 0.743\n",
      "176-th round\n",
      "average train loss 0.0327 | test loss 0.729 | test acc: 0.743\n",
      "177-th round\n",
      "average train loss 0.0408 | test loss 0.561 | test acc: 0.816\n",
      "178-th round\n",
      "average train loss 0.024 | test loss 0.598 | test acc: 0.785\n",
      "179-th round\n",
      "average train loss 0.0485 | test loss 0.56 | test acc: 0.818\n",
      "180-th round\n",
      "average train loss 0.0263 | test loss 0.829 | test acc: 0.719\n",
      "181-th round\n",
      "average train loss 0.0352 | test loss 0.99 | test acc: 0.650\n",
      "182-th round\n",
      "average train loss 0.0173 | test loss 0.566 | test acc: 0.799\n",
      "183-th round\n",
      "average train loss 0.0263 | test loss 0.587 | test acc: 0.783\n",
      "184-th round\n",
      "average train loss 0.11 | test loss 0.81 | test acc: 0.715\n",
      "185-th round\n",
      "average train loss 0.0406 | test loss 0.652 | test acc: 0.758\n",
      "186-th round\n",
      "average train loss 0.0343 | test loss 0.472 | test acc: 0.859\n",
      "187-th round\n",
      "average train loss 0.0553 | test loss 0.508 | test acc: 0.844\n",
      "188-th round\n",
      "average train loss 0.0201 | test loss 0.564 | test acc: 0.819\n",
      "189-th round\n",
      "average train loss 0.033 | test loss 0.452 | test acc: 0.865\n",
      "190-th round\n",
      "average train loss 0.0371 | test loss 0.616 | test acc: 0.789\n",
      "191-th round\n",
      "average train loss 0.0229 | test loss 0.567 | test acc: 0.813\n",
      "192-th round\n",
      "average train loss 0.239 | test loss 0.849 | test acc: 0.725\n",
      "193-th round\n",
      "average train loss 0.0402 | test loss 0.963 | test acc: 0.675\n",
      "194-th round\n",
      "average train loss 0.0623 | test loss 0.803 | test acc: 0.707\n",
      "195-th round\n",
      "average train loss 0.0434 | test loss 0.614 | test acc: 0.805\n",
      "196-th round\n",
      "average train loss 0.0356 | test loss 0.534 | test acc: 0.842\n",
      "197-th round\n",
      "average train loss 0.0831 | test loss 0.531 | test acc: 0.850\n",
      "198-th round\n",
      "average train loss 0.0797 | test loss 0.631 | test acc: 0.796\n",
      "199-th round\n",
      "average train loss 0.041 | test loss 0.553 | test acc: 0.825\n"
     ]
    }
   ],
   "source": [
    "# NON-IID case: every client has images of two categories chosen from [0, 1], [2, 3], [4, 5], [6, 7], or [8, 9].\n",
    "\n",
    "# Hyperparameters\n",
    "\n",
    "num_clients = 100\n",
    "num_selected = 10\n",
    "num_rounds = 100\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "local_ep_list = np.random.choice(range(1,epochs+1),size=num_clients)\n",
    "lr = 0.01\n",
    "# Creating decentralized datasets\n",
    "\n",
    "traindata = datasets.MNIST('./data', train=True, download=True,\n",
    "                       transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])\n",
    "                       )\n",
    "target_labels = torch.stack([traindata.targets == i for i in range(10)])\n",
    "target_labels_split = []\n",
    "for i in range(5):\n",
    "    target_labels_split += torch.split(torch.where(target_labels[(2 * i):(2 * (i + 1))].sum(0))[0], int(60000 / num_clients))\n",
    "traindata_split = [torch.utils.data.Subset(traindata, tl) for tl in target_labels_split]\n",
    "train_loader = [torch.utils.data.DataLoader(x, batch_size=batch_size, shuffle=True) for x in traindata_split]\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('./data', train=False, transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])\n",
    "        ), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Instantiate models and optimizers\n",
    "\n",
    "global_model = Net().cuda()\n",
    "client_models = [Net().cuda() for _ in range(num_selected)]\n",
    "for model in client_models:\n",
    "    model.load_state_dict(global_model.state_dict())\n",
    "# print(global_model.state_dict())\n",
    "\n",
    "opt = [optim.SGD(model.parameters(), lr=lr for model in client_models]\n",
    "# opt = optim.SGD(model.parameters(), lr=0.01) \n",
    "# Runnining FL\n",
    "p_initial = np.ones(num_clients)/num_clients # initialize the probability vector\n",
    "p_usersampling = p_initial\n",
    "        \n",
    "test_loss_accu[]\n",
    "acc_accu = []\n",
    "for r in range(num_rounds):\n",
    "    # select random clients\n",
    "#     client_idx = np.random.permutation(num_clients)[:num_selected]\n",
    "    client_idx = np.random.choice(range(num_clients), num_selected, replace=False,p = p_usersampling)\n",
    " \n",
    "    # client update\n",
    "#     loss = 0\n",
    "#     grad_client = 0\n",
    "    grad_list=[]\n",
    "    loss_list=[]\n",
    "    for i in range(num_selected):\n",
    "        loss,grad_client = client_update(client_models[i], opt[i], train_loader[client_idx[i]], \n",
    "                              epoch=int(local_ep_list[client_idx[i]]), num_clients=num_clients, pk=p_usersampling[client_idx[i]], batch_size=batch_size )\n",
    "        grad_list.append(grad_client)\n",
    "        loss_list.append(loss)\n",
    "    loss = sum(loss_list)\n",
    "    \n",
    "    grad_list = [a/sum(grad_list) for a in grad_list]\n",
    "    normalizing_factor = sum([p_usersampling[i] for i in client_idx])\n",
    "    \n",
    "    for i in range(num_selected):\n",
    "        p_usersampling[client_idx[i]]=(grad_list[i]/sum(grad_list)) * normalizing_factor\n",
    "    # serer aggregate\n",
    "    server_aggregate(global_model, client_models)\n",
    "    test_loss, acc = test(global_model, test_loader)\n",
    "    test_loss_accu.append(test_loss)\n",
    "    acc_accu.append(acc)\n",
    "                 \n",
    "    print('%d-th round' % r)\n",
    "    print('average train loss %0.3g | test loss %0.3g | test acc: %0.3f' % (loss / num_selected, test_loss, acc))\n",
    "#     print('sampling probability:' )\n",
    "#     print(p_usersampling)\n",
    "                 \n",
    "file_name = './save/objects/fedsample_Epoch{}_lr{}_loss_and_acc.pkl'.\n",
    "                format(epochs, lr)\n",
    "with open(file_name, 'wb') as f:\n",
    "            pickle.dump([test_loss_accu, acc_accu.append], f)\n",
    "                 \n",
    "# Plot Average Accuracy vs Communication rounds\n",
    "plt.figure()\n",
    "plt.title('Accuracy vs Communication rounds')\n",
    "plt.plot(range(len(acc_accu)), acc_accu, color='k')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Communication Rounds')\n",
    "plt.savefig('./save/fedsample_Epoch{}_lr{}_acc.png'.\n",
    "                format(epochs, lr))             \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
