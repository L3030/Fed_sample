{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]  =  \"TRUE\"\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import argparse\n",
    "# import logging\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.use('Agg')\n",
    "%matplotlib inline\n",
    "\n",
    "from random import Random\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.utils.data.distributed\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.multiprocessing import Process\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "class Partition(object):\n",
    "    \"\"\" Dataset-like object, but only access a subset of it. \"\"\"\n",
    "\n",
    "    def __init__(self, data, index):\n",
    "        self.data = data\n",
    "        self.index = index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_idx = self.index[index]\n",
    "        return self.data[data_idx]\n",
    "\n",
    "class DataPartitioner(object):\n",
    "    \"\"\" Partitions a dataset into different chuncks. \"\"\"\n",
    "    def __init__(self, data, sizes=[0.7, 0.2, 0.1], seed=1234, isNonIID=False, alpha=0, dataset=None):\n",
    "        self.data = data\n",
    "        self.dataset = dataset\n",
    "        if isNonIID:\n",
    "            self.partitions, self.ratio = self.__getDirichletData__(data, sizes, seed, alpha)\n",
    "\n",
    "        else:\n",
    "            self.partitions = [] \n",
    "            self.ratio = sizes\n",
    "            rng = Random() \n",
    "            rng.seed(seed) \n",
    "            data_len = len(data) \n",
    "            indexes = [x for x in range(0, data_len)] \n",
    "            rng.shuffle(indexes) \n",
    "             \n",
    "     \n",
    "            for frac in sizes: \n",
    "                part_len = int(frac * data_len)\n",
    "                self.partitions.append(indexes[0:part_len])\n",
    "                indexes = indexes[part_len:]\n",
    "\n",
    "        \n",
    "\n",
    "    def use(self, partition):\n",
    "        return Partition(self.data, self.partitions[partition])\n",
    "\n",
    "    def __getNonIIDdata__(self, data, sizes, seed, alpha):\n",
    "        labelList = data.train_labels\n",
    "        rng = Random()\n",
    "        rng.seed(seed)\n",
    "        a = [(label, idx) for idx, label in enumerate(labelList)]\n",
    "        # Same Part\n",
    "        labelIdxDict = dict()\n",
    "        for label, idx in a:\n",
    "            labelIdxDict.setdefault(label,[])\n",
    "            labelIdxDict[label].append(idx)\n",
    "        labelNum = len(labelIdxDict)\n",
    "        labelNameList = [key for key in labelIdxDict]\n",
    "        labelIdxPointer = [0] * labelNum\n",
    "        # sizes = number of nodes\n",
    "        partitions = [list() for i in range(len(sizes))]\n",
    "        eachPartitionLen= int(len(labelList)/len(sizes))\n",
    "        # majorLabelNumPerPartition = ceil(labelNum/len(partitions))\n",
    "        majorLabelNumPerPartition = 2\n",
    "        basicLabelRatio = alpha\n",
    "\n",
    "        interval = 1\n",
    "        labelPointer = 0\n",
    "\n",
    "        #basic part\n",
    "        for partPointer in range(len(partitions)):\n",
    "            requiredLabelList = list()\n",
    "            for _ in range(majorLabelNumPerPartition):\n",
    "                requiredLabelList.append(labelPointer)\n",
    "                labelPointer += interval\n",
    "                if labelPointer > labelNum - 1:\n",
    "                    labelPointer = interval\n",
    "                    interval += 1\n",
    "            for labelIdx in requiredLabelList:\n",
    "                start = labelIdxPointer[labelIdx]\n",
    "                idxIncrement = int(basicLabelRatio*len(labelIdxDict[labelNameList[labelIdx]]))\n",
    "                partitions[partPointer].extend(labelIdxDict[labelNameList[labelIdx]][start:start+ idxIncrement])\n",
    "                labelIdxPointer[labelIdx] += idxIncrement\n",
    "\n",
    "        #random part\n",
    "        remainLabels = list()\n",
    "        for labelIdx in range(labelNum):\n",
    "            remainLabels.extend(labelIdxDict[labelNameList[labelIdx]][labelIdxPointer[labelIdx]:])\n",
    "        rng.shuffle(remainLabels)\n",
    "        for partPointer in range(len(partitions)):\n",
    "            idxIncrement = eachPartitionLen - len(partitions[partPointer])\n",
    "            partitions[partPointer].extend(remainLabels[:idxIncrement])\n",
    "            rng.shuffle(partitions[partPointer])\n",
    "            remainLabels = remainLabels[idxIncrement:]\n",
    "\n",
    "        return partitions\n",
    "\n",
    "    def __getDirichletData__(self, data, psizes, seed, alpha):\n",
    "        n_nets = len(psizes)\n",
    "        K = 10\n",
    "        labelList = np.array(data.targets)\n",
    "        min_size = 0\n",
    "        N = len(labelList)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        net_dataidx_map = {}\n",
    "        while min_size < K:\n",
    "            idx_batch = [[] for _ in range(n_nets)]\n",
    "            # for each class in the dataset\n",
    "            for k in range(K):\n",
    "                idx_k = np.where(labelList == k)[0]\n",
    "                np.random.shuffle(idx_k)\n",
    "                proportions = np.random.dirichlet(np.repeat(alpha, n_nets))\n",
    "                ## Balance\n",
    "                proportions = np.array([p*(len(idx_j)<N/n_nets) for p,idx_j in zip(proportions,idx_batch)])\n",
    "                proportions = proportions/proportions.sum()\n",
    "                proportions = (np.cumsum(proportions)*len(idx_k)).astype(int)[:-1]\n",
    "                idx_batch = [idx_j + idx.tolist() for idx_j,idx in zip(idx_batch,np.split(idx_k,proportions))]\n",
    "                min_size = min([len(idx_j) for idx_j in idx_batch])\n",
    "\n",
    "        for j in range(n_nets):\n",
    "            np.random.shuffle(idx_batch[j])\n",
    "            net_dataidx_map[j] = idx_batch[j]\n",
    "            \n",
    "        net_cls_counts = {}\n",
    "\n",
    "        for net_i, dataidx in net_dataidx_map.items():\n",
    "            unq, unq_cnt = np.unique(labelList[dataidx], return_counts=True)\n",
    "            tmp = {unq[i]: unq_cnt[i] for i in range(len(unq))}\n",
    "            net_cls_counts[net_i] = tmp\n",
    "        print('Data statistics: %s' % str(net_cls_counts))\n",
    "\n",
    "        local_sizes = []\n",
    "        for i in range(n_nets):\n",
    "            local_sizes.append(len(net_dataidx_map[i]))\n",
    "        local_sizes = np.array(local_sizes)\n",
    "        weights = local_sizes/np.sum(local_sizes)\n",
    "        print(weights)\n",
    "\n",
    "        return idx_batch, weights\n",
    "\n",
    "def partition_dataset(size, args_alpha):\n",
    "    print('==> load train data')\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data/',\n",
    "                                            train=True, \n",
    "                                            download=True, \n",
    "                                            transform=transform_train)\n",
    "    \n",
    "    partition_sizes = [1.0 / size for _ in range(size)]\n",
    "    partition = DataPartitioner(trainset, partition_sizes, isNonIID=True, alpha=args_alpha)\n",
    "    ratio = partition.ratio\n",
    "#     partition = partition.use(rank)\n",
    "#     train_loader = torch.utils.data.DataLoader(partition, \n",
    "#                                             batch_size=32, \n",
    "#                                             shuffle=True, \n",
    "#                                             )\n",
    "\n",
    "\n",
    "    partitions = partition.partitions\n",
    "    train_sets = []\n",
    "    for k in range(size):\n",
    "        local_partition = Partition(trainset, partitions[k])\n",
    "        train_sets.append(local_partition)\n",
    "\n",
    "    \n",
    "        \n",
    "    print('==> load test data')\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    testset = torchvision.datasets.CIFAR10(root='./data/', \n",
    "                                        train=False, \n",
    "                                        download=True, \n",
    "                                        transform=transform_test)\n",
    "        \n",
    "    \n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(testset, \n",
    "                                            batch_size=32, #64\n",
    "                                            shuffle=False, \n",
    "                                            )\n",
    "\n",
    "    # You can add more datasets here\n",
    "    return train_sets, test_loader, ratio\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     parser = argparse.ArgumentParser(description='CIFAR-10 baseline')\n",
    "#     parser.add_argument('--client_num','-cN', \n",
    "#                     default=10, \n",
    "#                     type=int, \n",
    "#                     help='the number of clients')\n",
    "#     parser.add_argument('--round_num','-rN', \n",
    "#                     default=10, \n",
    "#                     type=int, \n",
    "#                     help='the number of communication rounds')\n",
    "#     parser.add_argument('--round_drift','-rd', \n",
    "#                     default=1, \n",
    "#                     type=float, \n",
    "#                     help='round drift') \n",
    "#     parser.add_argument('--client_drift','-cd', \n",
    "#                     default=0.1, \n",
    "#                     type=float, \n",
    "#                     help='client drift')\n",
    "#     parser.add_argument('--lr', \n",
    "#                     default=0.1, \n",
    "#                     type=float, \n",
    "#                     help='client learning rate')\n",
    "#     parser.add_argument('--rank', \n",
    "#                     default=0, \n",
    "#                     type=int, \n",
    "#                     help='the rank of worker')\n",
    "\n",
    "#?????????bs 是啥\n",
    "\n",
    "#     parser.add_argument('--bs', \n",
    "#                     default=32, \n",
    "#                     type=int, \n",
    "#                     help='batch size on each worker/client')\n",
    "#     parser.add_argument('--NIID',\n",
    "#                     default=True,\n",
    "#                     action='store_true',\n",
    "#                     help='whether the dataset is non-iid or not')\n",
    "#     parser.add_argument('--datapath',\n",
    "#                     default='./data/',\n",
    "#                     type=str,\n",
    "#                     help='directory to load data')\n",
    "#     args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.fc1 = nn.Linear(2048, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "\n",
    "def client_update(client_model, optimizer, train_loader, epoch, num_clients, pk, batch_size):\n",
    "    client_model.train()\n",
    "    Grad_accumulator = []\n",
    "    for e in range(epoch):\n",
    "        grad_batch_idx = []\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            output = client_model(data)\n",
    "            loss_fed = F.nll_loss(output, target)\n",
    "            loss = loss_fed/(epoch )\n",
    "            loss.backward()\n",
    "           \n",
    "            if Grad_accumulator == []:\n",
    "                Grad_accumulator = list(i.grad for i in list(client_model.parameters()))        \n",
    "            else:\n",
    "                h = list(i.grad for i in list(client_model.parameters()))\n",
    "                Grad_accumulator = [Grad_accumulator[i]+h[i] for i in range(len(Grad_accumulator))]\n",
    "            optimizer.step()\n",
    "\n",
    "    \n",
    "    nabla_P_norm2 = sum([(torch.norm(a))**2 for a in Grad_accumulator]).item()\n",
    "    grad_client = (1/(batch_size))*np.sqrt(nabla_P_norm2)\n",
    "    return loss_fed.item(),grad_client\n",
    "\n",
    "\n",
    "def client_update_fed(client_model, optimizer, train_loader, epoch):\n",
    "    client_model.train()\n",
    "    for e in range(epoch):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            output = client_model(data)\n",
    "            loss = F.nll_loss(output, target)/epoch\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def server_aggregate(global_model, client_models, num_clients, pk, client_index):\n",
    "    global_dict = global_model.state_dict()\n",
    "    for k in global_dict.keys():\n",
    "        global_dict[k] = torch.stack([client_models[i].state_dict()[k]/(num_clients*pk[client_index[i]]) for i in range(len(client_models))], 0).mean(0)\n",
    "    global_model.load_state_dict(global_dict)\n",
    "    for model in client_models:\n",
    "        model.load_state_dict(global_model.state_dict())\n",
    "\n",
    "def test(global_model, test_loader):\n",
    "    global_model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            output = global_model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    acc = correct / len(test_loader.dataset)\n",
    "\n",
    "    return test_loss, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> load train data\n",
      "Files already downloaded and verified\n",
      "Data statistics: {0: {3: 1, 4: 1017}, 1: {0: 366, 1: 212}, 2: {2: 315, 6: 117, 8: 93}, 3: {0: 1, 2: 1011}, 4: {1: 293, 2: 34, 3: 5, 5: 115, 8: 228}, 5: {1: 5, 2: 8, 3: 1139}, 6: {0: 93, 6: 50, 8: 1}, 7: {1: 107, 2: 2, 3: 1, 8: 329}, 8: {0: 206, 2: 1, 4: 332}, 9: {1: 1, 2: 7, 3: 4, 5: 3, 8: 1015}, 10: {0: 372, 2: 1, 4: 273}, 11: {0: 3, 1: 94, 2: 2, 8: 30}, 12: {0: 15, 2: 10, 3: 1, 4: 1, 5: 23, 6: 997}, 13: {0: 25, 4: 86, 8: 4, 9: 4}, 14: {0: 3, 1: 5, 4: 7, 7: 8, 8: 7}, 15: {0: 1, 2: 1, 3: 29, 5: 4, 6: 1, 7: 36, 9: 7}, 16: {0: 1, 1: 15, 2: 6, 3: 41, 4: 11, 8: 23, 9: 5}, 17: {3: 39, 4: 85, 5: 180, 6: 10, 7: 1, 8: 121}, 18: {2: 186, 4: 1, 5: 21, 7: 140}, 19: {1: 1, 3: 7, 7: 10}, 20: {0: 25, 1: 4, 3: 105, 5: 27, 6: 3, 9: 28}, 21: {0: 45, 1: 415, 2: 28, 5: 2, 6: 300}, 22: {1: 2, 2: 13, 3: 206, 5: 27, 8: 4}, 23: {0: 22, 3: 1, 4: 143, 5: 5, 6: 33, 7: 6, 9: 1}, 24: {0: 46, 2: 6, 5: 632}, 25: {1: 1, 2: 1, 3: 5, 4: 158, 5: 3, 7: 54, 8: 49, 9: 93}, 26: {0: 9, 1: 180, 2: 377}, 27: {0: 1, 1: 288, 3: 207, 7: 84}, 28: {3: 2, 5: 6, 7: 74, 8: 1186}, 29: {0: 1, 3: 12, 4: 52, 5: 11, 6: 409, 8: 1, 9: 17}, 30: {0: 15, 1: 1, 3: 23, 5: 4, 6: 525}, 31: {0: 20, 2: 54, 6: 385, 7: 1, 8: 209}, 32: {0: 209, 2: 203, 5: 34, 7: 46, 9: 174}, 33: {5: 1, 6: 6, 8: 4}, 34: {4: 3, 6: 1, 9: 15}, 35: {2: 5, 3: 1, 4: 531}, 36: {0: 15, 2: 95, 3: 22, 4: 52, 5: 20, 9: 43}, 37: {0: 56, 2: 129, 5: 1, 7: 16, 8: 231, 9: 181}, 38: {2: 182, 6: 1, 8: 13}, 39: {0: 6, 1: 1, 3: 1, 4: 12, 7: 14, 8: 534}, 40: {0: 24, 2: 4, 3: 3, 8: 7}, 41: {0: 436, 1: 2, 2: 385}, 42: {1: 2, 2: 5, 3: 15, 4: 916}, 43: {1: 124, 3: 11, 5: 151, 6: 195, 9: 100}, 44: {1: 90, 5: 9, 7: 4}, 45: {0: 3, 2: 28, 4: 64, 7: 33}, 46: {0: 4, 1: 73, 4: 2, 5: 41, 6: 1579}, 47: {0: 8, 1: 11, 3: 1, 5: 2, 7: 2}, 48: {0: 106, 1: 1, 2: 5, 4: 13, 9: 16}, 49: {0: 33, 1: 4, 2: 1, 4: 2, 5: 1, 8: 14}, 50: {2: 5, 3: 406, 5: 1, 9: 2}, 51: {0: 54, 1: 1, 2: 487}, 52: {1: 223, 3: 33, 4: 131, 5: 19, 7: 673}, 53: {0: 216, 5: 335}, 54: {0: 371, 3: 2, 5: 201}, 55: {0: 1, 5: 372, 6: 32, 7: 83, 8: 1, 9: 17}, 56: {0: 10, 2: 7, 3: 1, 7: 26, 8: 5, 9: 1051}, 57: {0: 26, 1: 2, 3: 826}, 58: {0: 2, 1: 1, 5: 265, 6: 7, 7: 81, 9: 417}, 59: {3: 7, 5: 41, 6: 17, 7: 1, 8: 39, 9: 62}, 60: {0: 235, 2: 362}, 61: {0: 30, 2: 1, 4: 73, 7: 623}, 62: {0: 134, 1: 2, 2: 1, 3: 108, 4: 15, 7: 4, 8: 18, 9: 1}, 63: {0: 97, 1: 87, 4: 2, 5: 1}, 64: {0: 9, 3: 606}, 65: {0: 586}, 66: {0: 17, 1: 309, 4: 1, 7: 22, 8: 303}, 67: {0: 165, 1: 411}, 68: {0: 75, 1: 1, 4: 89, 5: 4, 7: 4, 8: 4}, 69: {0: 11, 1: 5, 3: 1, 9: 366}, 70: {1: 7, 3: 453, 5: 4, 7: 915}, 71: {2: 12, 3: 73, 6: 18, 7: 58, 9: 257}, 72: {5: 41, 6: 45, 7: 92, 9: 62}, 73: {3: 49, 7: 3, 9: 78}, 74: {3: 172, 4: 5, 5: 888}, 75: {1: 57, 3: 167, 5: 201, 6: 2, 9: 1}, 76: {2: 222, 3: 1, 4: 51, 8: 8}, 77: {0: 4, 2: 49, 3: 5, 4: 5, 5: 8, 8: 20}, 78: {4: 13, 8: 1}, 79: {0: 90, 2: 208, 3: 1, 9: 512}, 80: {1: 6, 2: 32, 4: 249, 5: 578}, 81: {1: 1, 4: 94, 5: 34, 9: 37}, 82: {0: 1, 1: 5, 4: 1, 5: 2, 7: 5, 9: 124}, 83: {0: 395, 1: 554}, 84: {3: 38, 4: 2, 8: 233, 9: 239}, 85: {1: 14, 2: 351, 4: 2, 5: 119, 9: 48}, 86: {0: 31, 4: 28, 5: 55, 6: 55}, 87: {1: 59, 2: 4, 3: 33, 4: 2, 5: 162, 6: 46, 8: 250}, 88: {0: 143, 2: 4, 3: 16, 5: 80, 9: 861}, 89: {4: 179, 5: 265, 7: 1, 8: 2}, 90: {1: 1, 3: 2, 4: 8, 6: 141, 7: 1346}, 91: {0: 5, 2: 62, 3: 2, 4: 1, 8: 4}, 92: {0: 96, 2: 10, 3: 3, 4: 160, 7: 25, 9: 3}, 93: {1: 4, 2: 17, 3: 103, 4: 63, 6: 9}, 94: {2: 39, 7: 3}, 95: {0: 1, 2: 16, 4: 55, 7: 43, 8: 8, 9: 162}, 96: {2: 5, 3: 4, 4: 7, 7: 1}, 97: {0: 6, 1: 10, 3: 5, 4: 2, 9: 15}, 98: {0: 18, 1: 1307}, 99: {0: 1, 1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 16, 7: 462, 8: 1, 9: 1}}\n",
      "[0.02036 0.01156 0.0105  0.02024 0.0135  0.02304 0.00288 0.00878 0.01078\n",
      " 0.0206  0.01292 0.00258 0.02094 0.00238 0.0006  0.00158 0.00204 0.00872\n",
      " 0.00696 0.00036 0.00384 0.0158  0.00504 0.00422 0.01368 0.00728 0.01132\n",
      " 0.0116  0.02536 0.01006 0.01136 0.01338 0.01332 0.00022 0.00038 0.01074\n",
      " 0.00494 0.01228 0.00392 0.01136 0.00076 0.01646 0.01876 0.01162 0.00206\n",
      " 0.00256 0.03398 0.00048 0.00282 0.0011  0.00828 0.01084 0.02158 0.01102\n",
      " 0.01148 0.01012 0.022   0.01708 0.01546 0.00334 0.01194 0.01454 0.00566\n",
      " 0.00374 0.0123  0.01172 0.01304 0.01152 0.00354 0.00766 0.02758 0.00836\n",
      " 0.0048  0.0026  0.0213  0.00856 0.00564 0.00182 0.00028 0.01622 0.0173\n",
      " 0.00332 0.00276 0.01898 0.01024 0.01068 0.00338 0.01112 0.02208 0.00894\n",
      " 0.02996 0.00148 0.00594 0.00392 0.00084 0.0057  0.00034 0.00076 0.0265\n",
      " 0.00972]\n",
      "==> load test data\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'cuda'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d65dda73786a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mloss_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_selected\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         loss,grad_client = client_update(client_models[i], opt[i], train_loader[client_idx[i]], \n\u001b[0m\u001b[1;32m     51\u001b[0m                               epoch=int(local_ep_list[client_idx[i]]), num_clients=num_clients, pk=p_usersampling[client_idx[i]], batch_size=batch_size )\n\u001b[1;32m     52\u001b[0m         \u001b[0mgrad_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-45b69f9b7cb5>\u001b[0m in \u001b[0;36mclient_update\u001b[0;34m(client_model, optimizer, train_loader, epoch, num_clients, pk, batch_size)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mgrad_batch_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'cuda'"
     ]
    }
   ],
   "source": [
    "# NON-IID case: every client has images of two categories chosen from [0, 1], [2, 3], [4, 5], [6, 7], or [8, 9].\n",
    "\n",
    "args_alpha = 0.1\n",
    "# Hyperparameters\n",
    "size = 100 #size = num_clients\n",
    "num_clients = 100\n",
    "num_selected = 10\n",
    "num_rounds = 400\n",
    "epochs = 6\n",
    "batch_size = 32\n",
    "local_ep_list = np.random.choice(range(1,epochs+1),size=num_clients)\n",
    "lr = 0.01\n",
    "\n",
    "# Creating decentralized datasets\n",
    "train_loader, test_loader, DataRatios = \\\n",
    "        partition_dataset(size, args_alpha)\n",
    "\n",
    "# print(\"==========  This is train_loader:  =============\")\n",
    "# print(type(train_loader[0]))\n",
    "# fn = getattr(train_loader[0], '__main__')\n",
    "# print(fn())\n",
    "# print(train_loader)\n",
    "\n",
    "# Instantiate models and optimizers\n",
    "\n",
    "global_model = Net().cuda()\n",
    "client_models = [Net().cuda() for _ in range(num_selected)]\n",
    "for model in client_models:\n",
    "    model.load_state_dict(global_model.state_dict())\n",
    "# print(global_model.state_dict())\n",
    "\n",
    "opt = [optim.SGD(model.parameters(), lr=lr) for model in client_models]\n",
    "# opt = optim.SGD(model.parameters(), lr=0.01) \n",
    "                 \n",
    "# Runnining FL\n",
    "p_initial = np.ones(num_clients)/num_clients # initialize the probability vector\n",
    "p_usersampling = p_initial\n",
    "        \n",
    "test_loss_accu=[]\n",
    "acc_accu = []\n",
    "for r in range(num_rounds):\n",
    "    # select random clients\n",
    "#     client_idx = np.random.permutation(num_clients)[:num_selected]\n",
    "    client_idx = np.random.choice(range(num_clients), num_selected, replace=False,p = p_usersampling)\n",
    " \n",
    "    # client update\n",
    "    grad_list=[]\n",
    "    loss_list=[]\n",
    "    for i in range(num_selected):\n",
    "        loss,grad_client = client_update(client_models[i], opt[i], train_loader[client_idx[i]], \n",
    "                              epoch=int(local_ep_list[client_idx[i]]), num_clients=num_clients, pk=p_usersampling[client_idx[i]], batch_size=batch_size )\n",
    "        grad_list.append(grad_client)\n",
    "        loss_list.append(loss)\n",
    "    loss = sum(loss_list)\n",
    "    # serer aggregate\n",
    "    server_aggregate(global_model, client_models, num_clients=num_clients, pk=p_usersampling , client_index=client_index )\n",
    "    #update sample prob\n",
    "    grad_list = [a/sum(grad_list) for a in grad_list]\n",
    "    normalizing_factor = sum([p_usersampling[i] for i in client_idx])\n",
    "    \n",
    "    for i in range(num_selected):\n",
    "        p_usersampling[client_idx[i]]=(grad_list[i]/sum(grad_list)) * normalizing_factor\n",
    "    \n",
    "    \n",
    "    test_loss, acc = test(global_model, test_loader)\n",
    "    test_loss_accu.append(test_loss)\n",
    "    acc_accu.append(acc)\n",
    "                 \n",
    "    print('%d-th round' % r)\n",
    "    print('average train loss %0.3g | test loss %0.3g | test acc: %0.3f' % (loss / num_selected, test_loss, acc))\n",
    "#     print('sampling probability:' )\n",
    "#     print(p_usersampling)\n",
    "                 \n",
    "file_name = './save/objects/fedsample_Epoch{}_lr{}_round{}_loss_and_acc.pkl'. \\\n",
    "                format(epochs, lr,num_rounds)\n",
    "with open(file_name, 'wb') as f:\n",
    "            pickle.dump([test_loss_accu, acc_accu.append], f)\n",
    "                 \n",
    "# Plot Average Accuracy vs Communication rounds\n",
    "plt.figure()\n",
    "plt.title('Accuracy vs Communication rounds')\n",
    "plt.plot(range(len(acc_accu)), acc_accu, color='k')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Communication Rounds')\n",
    "plt.savefig('./save/fedsample_diffEpoch{}_lr{}_round{}_acc.png'.\n",
    "                format(epochs, lr, num_rounds))             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NON-IID case: every client has images of two categories chosen from [0, 1], [2, 3], [4, 5], [6, 7], or [8, 9].\n",
    "\n",
    "args_alpha = 0.1\n",
    "# Hyperparameters\n",
    "size = 100 #size = num_selected\n",
    "num_clients = 100\n",
    "num_selected = 10\n",
    "num_rounds = 400\n",
    "epochs = 6\n",
    "batch_size = 32\n",
    "local_ep_list = np.random.choice(range(1,epochs+1),size=num_clients)\n",
    "lr = 0.01\n",
    "# Creating decentralized datasets\n",
    "\n",
    "# traindata = datasets.MNIST('./data', train=True, download=True,\n",
    "#                        transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])\n",
    "#                        )\n",
    "# target_labels = torch.stack([traindata.targets == i for i in range(10)])\n",
    "# target_labels_split = []\n",
    "# for i in range(5):\n",
    "#     target_labels_split += torch.split(torch.where(target_labels[(2 * i):(2 * (i + 1))].sum(0))[0], int(60000 / num_clients))\n",
    "# traindata_split = [torch.utils.data.Subset(traindata, tl) for tl in target_labels_split]\n",
    "# train_loader = [torch.utils.data.DataLoader(x, batch_size=batch_size, shuffle=True) for x in traindata_split]\n",
    "\n",
    "# test_loader = torch.utils.data.DataLoader(\n",
    "#         datasets.MNIST('./data', train=False, transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])\n",
    "#         ), batch_size=batch_size, shuffle=True)\n",
    "train_loader, test_loader, DataRatios = \\\n",
    "        partition_dataset( size, args_alpha)\n",
    "# logging.debug(\"Worker id {} local sample ratio {} \"\n",
    "#               \"local epoch length {}\"\n",
    "#               .format(rank, DataRatios[rank], len(train_loader)))\n",
    "\n",
    "# Instantiate models and optimizers\n",
    "\n",
    "global_model = Net().cuda()\n",
    "client_models = [Net().cuda() for _ in range(num_selected)]\n",
    "for model in client_models:\n",
    "    model.load_state_dict(global_model.state_dict())\n",
    "# print(global_model.state_dict())\n",
    "\n",
    "\n",
    "opt = [optim.SGD(model.parameters(), lr=lr) for model in client_models]\n",
    "\n",
    "# Runnining FL\n",
    "test_loss_accu1 = []\n",
    "acc_accu1 = []\n",
    "for r in range(num_rounds):\n",
    "    # select random clients\n",
    "    client_idx = np.random.permutation(num_clients)[:num_selected]\n",
    "\n",
    "    # client update\n",
    "    loss = 0\n",
    "    for i in range(num_selected):\n",
    "        loss += client_update_fed(client_models[i], opt[i], train_loader[client_idx[i]], epoch=int(local_ep_list[client_idx[i]]))\n",
    "    \n",
    "    # serer aggregate\n",
    "    server_aggregate(global_model, client_models)\n",
    "    test_loss, acc = test(global_model, test_loader)\n",
    "    test_loss_accu1.append(test_loss)\n",
    "    acc_accu1.append(acc)\n",
    "    \n",
    "    print('%d-th round' % r)\n",
    "    print('average train loss %0.3g | test loss %0.3g | test acc: %0.3f' % (loss / num_selected, test_loss, acc))\n",
    "    \n",
    "file_name = './save/objects/fed_diffEpoch{}_lr{}_round{}_loss_and_acc.pkl'. \\\n",
    "                format(epochs, lr, num_rounds)\n",
    "with open(file_name, 'wb') as f:\n",
    "            pickle.dump([test_loss_accu1, acc_accu1], f)\n",
    "    \n",
    "# Plot Average Accuracy vs Communication rounds\n",
    "plt.figure()\n",
    "plt.title('Accuracy vs Communication rounds')\n",
    "plt.plot(range(len(acc_accu1)), acc_accu1, color='k')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Communication Rounds')\n",
    "plt.savefig('./save/fed_diffEpoch{}_lr{}_round{}_acc.png'.\n",
    "                format(epochs, lr, num_rounds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NON-IID case: every client has images of two categories chosen from [0, 1], [2, 3], [4, 5], [6, 7], or [8, 9].\n",
    "\n",
    "args_alpha = 0.1\n",
    "# Hyperparameters\n",
    "size = 100 #size = num_selected\n",
    "num_clients = 100\n",
    "num_selected = 10\n",
    "num_rounds = 400\n",
    "epochs = 3\n",
    "batch_size = 32\n",
    "lr = 0.01\n",
    "\n",
    "# Creating decentralized datasets\n",
    "\n",
    "# traindata = datasets.MNIST('./data', train=True, download=True,\n",
    "#                        transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])\n",
    "#                        )\n",
    "# target_labels = torch.stack([traindata.targets == i for i in range(10)])\n",
    "# target_labels_split = []\n",
    "# for i in range(5):\n",
    "#     target_labels_split += torch.split(torch.where(target_labels[(2 * i):(2 * (i + 1))].sum(0))[0], int(60000 / num_clients))\n",
    "# traindata_split = [torch.utils.data.Subset(traindata, tl) for tl in target_labels_split]\n",
    "# train_loader = [torch.utils.data.DataLoader(x, batch_size=batch_size, shuffle=True) for x in traindata_split]\n",
    "\n",
    "# test_loader = torch.utils.data.DataLoader(\n",
    "#         datasets.MNIST('./data', train=False, transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])\n",
    "#         ), batch_size=batch_size, shuffle=True)\n",
    "train_loader, test_loader, DataRatios = \\\n",
    "        partition_dataset( size, args_alpha)\n",
    "# logging.debug(\"Worker id {} local sample ratio {} \"\n",
    "#               \"local epoch length {}\"\n",
    "#               .format(rank, DataRatios[rank], len(train_loader)))\n",
    "\n",
    "# Instantiate models and optimizers\n",
    "\n",
    "global_model = Net().cuda()\n",
    "client_models = [Net().cuda() for _ in range(num_selected)]\n",
    "for model in client_models:\n",
    "    model.load_state_dict(global_model.state_dict())\n",
    "\n",
    "opt = [optim.SGD(model.parameters(), lr=lr) for model in client_models]\n",
    "\n",
    "# Runnining FL\n",
    "test_loss_accu2=[]\n",
    "acc_accu2 = []\n",
    "for r in range(num_rounds):\n",
    "    # select random clients\n",
    "    client_idx = np.random.permutation(num_clients)[:num_selected]\n",
    "\n",
    "    # client update\n",
    "    loss = 0\n",
    "    for i in range(num_selected):\n",
    "        loss += client_update_fed(client_models[i], opt[i], train_loader[client_idx[i]], epoch=epochs)\n",
    "    \n",
    "    # serer aggregate\n",
    "    server_aggregate(global_model, client_models)\n",
    "    test_loss, acc = test(global_model, test_loader)\n",
    "    test_loss_accu2.append(test_loss)\n",
    "    acc_accu2.append(acc)\n",
    "    \n",
    "    print('%d-th round' % r)\n",
    "    print('average train loss %0.3g | test loss %0.3g | test acc: %0.3f' % (loss / num_selected, test_loss, acc))\n",
    "    \n",
    "file_name = './save/objects/fed_sameEpoch{}_lr{}_round{}_loss_and_acc.pkl'. \\\n",
    "                format(epochs, lr, num_rounds)\n",
    "with open(file_name, 'wb') as f:\n",
    "            pickle.dump([test_loss_accu2, acc_accu2], f)\n",
    "\n",
    "\n",
    "# Plot Average Accuracy vs Communication rounds\n",
    "plt.figure()\n",
    "plt.title('Accuracy of Fed with same Epoch vs Communication rounds')\n",
    "plt.plot(range(len(acc_accu2)), acc_accu2, color='k')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Communication Rounds')\n",
    "plt.savefig('./save/fed_sameEpoch{}_lr{}_round{}_acc.png'.\n",
    "                format(epochs, lr, num_rounds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title('Acc vs Communication rounds')\n",
    "plt.plot(range(len(acc_accu)), acc_accu, color='r', label='importance')\n",
    "plt.plot(range(len(acc_accu1)), acc_accu1,color='b', label='uniform')\n",
    "plt.plot(range(len(acc_accu2)), acc_accu2,color='g', label='Fed-uniform-uniformE')\n",
    "plt.ylabel('Acc')\n",
    "plt.xlabel('Communication Rounds')\n",
    "plt.savefig('./save/fed_acc_Epoch{}_lr{}_round{}_acc.png'.\n",
    "                format(epochs, lr,num_rounds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_accu_tune = [acc_accu[i] for i in range(0,len(acc_accu),20)]\n",
    "acc_accu_tune1 = [acc_accu1[i] for i in range(0,len(acc_accu1),20)]\n",
    "acc_accu_tune2 = [acc_accu2[i] for i in range(0,len(acc_accu2),20)]\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Acc vs Communication rounds')\n",
    "plt.plot(range(len(acc_accu_tune)), acc_accu_tune, color='r', label='importance')\n",
    "plt.plot(range(len(acc_accu_tune1)), acc_accu_tune1,color='b', label='uniform')\n",
    "plt.plot(range(len(acc_accu_tune2)), acc_accu_tune2,color='g', label='Fed-uniform-uniformE')\n",
    "plt.ylabel('Acc')\n",
    "plt.xlabel('Communication Rounds')\n",
    "plt.savefig('./save/tune10_fed_acc_Epoch{}_lr{}_round{}_acc.png'.\n",
    "                format(epochs, lr, num_rounds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
